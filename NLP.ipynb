{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jasleen8801/Conversational-Systems/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPKEXm83CIg"
      },
      "source": [
        "## **NLTK**\n",
        "The Natural Language Toolkit (NLTK) is a powerful Python library for working with human language data, particularly for natural language processing (NLP) and text analysis tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClrB7_UexJgT",
        "outputId": "720fcd10-379b-4794-f8c3-569eebe2ed83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ5MWhk_ya-b"
      },
      "source": [
        "**NLTK Corpora**: NLTK Corpora refers to a collection of text datasets and resources that the Natural Language Toolkit (NLTK) library provides.\n",
        "\n",
        "**Text Processing**: Text Processing involves converting textual data into mathematical objects, typically vectors or matrices, that can be analyzed, manipulated, and used for various machine learning and statistical techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjsgGxi9ze00",
        "outputId": "f50c270f-32ee-4178-9fd0-91fe4fbd9b81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jasleen/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/jasleen/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /home/jasleen/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/jasleen/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7LMqa6O83mjB"
      },
      "outputs": [],
      "source": [
        "sentence = \"I am running in the park and playing with my friends.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZygyOIzM3d2g"
      },
      "source": [
        "**Tokenization**: Tokenization is a fundamental text processing task in natural language processing (NLP). It involves breaking a text into individual units, typically words or tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhOxN2gmxorA",
        "outputId": "2fb12ee0-6903-4994-93f4-a1c85554f968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'am', 'running', 'in', 'the', 'park', 'and', 'playing', 'with', 'my', 'friends', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIYS4Ek7zJc_"
      },
      "source": [
        "**Stop words removal**-  Stop words are words that are considered to be of little value in text analysis because they are very common and do not carry much information about the content of the text. Examples of stop words in English include \"the,\" \"and,\" \"in,\" \"is,\" and \"it.\" Removing stop words can help reduce noise in your text data and improve the efficiency and accuracy of various NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIkxHhLFzN5i",
        "outputId": "7faf4840-2a3b-4bc2-ee9c-e26c215f27e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'him', 'isn', 'themselves', 'should', 'he', 'the', 'from', 'when', 'each', 'between', \"wouldn't\", 'couldn', 'once', 'out', 'off', 'needn', 'hasn', 'be', \"you've\", 'same', 'doesn', \"weren't\", 'other', 'that', 'then', 'your', 'shouldn', 'there', 'above', \"aren't\", 'having', 'any', 'of', 'against', 'its', 'before', 'nor', 'where', 'further', 'why', 'on', 'again', \"shan't\", 'she', 'mightn', \"shouldn't\", 't', 'what', 'into', \"hadn't\", 'below', 'in', 'wasn', 'after', 'few', 'at', 'this', 'll', 'for', 'but', 'mustn', \"wasn't\", 'because', 'who', \"hasn't\", 'm', 'doing', \"that'll\", 'so', \"needn't\", \"don't\", 'himself', 'such', 'an', 'y', 'which', 'didn', 'was', \"you'd\", 'now', 'through', 'have', 'up', 'theirs', 'his', 'they', \"didn't\", 'shan', 'herself', 'hers', \"you'll\", 'can', \"mightn't\", 'you', 'being', 'only', 'about', 'will', 'd', 'were', \"isn't\", 'not', 'no', 'own', \"doesn't\", 'ain', 'over', 'under', \"it's\", 're', 'our', 'while', 'am', 'them', 'had', 'during', \"you're\", 's', 've', 'it', 'my', 'aren', 'yourselves', 'until', 'whom', 'yourself', 'with', 'and', 'all', 'a', 'as', 'too', 'won', 'ours', 'does', 'has', 'than', 'yours', 'their', 'haven', 'is', 'down', 'by', 'ma', 'both', 'o', 'just', 'these', 'her', \"haven't\", 'some', \"couldn't\", 'if', 'very', 'those', 'we', 'to', 'did', 'how', 'more', 'weren', 'here', 'myself', \"won't\", 'wouldn', 'i', 'don', 'me', 'been', 'or', 'itself', 'are', 'most', \"she's\", 'do', \"should've\", \"mustn't\", 'hadn', 'ourselves'}\n",
            "running park playing friends .\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "\n",
        "# print(tokens)\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "filtered_text = ' '.join(filtered_tokens)\n",
        "print(filtered_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmtRwKGVz73O"
      },
      "source": [
        "**Lemmatization**: Lemmatization reduces words to their base or root form, which can be helpful in various natural language processing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uny7JJKVzhRW",
        "outputId": "ff34f027-5bb4-44f3-fa6e-5950120be5fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am running in the park and playing with my friend .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "print(lemmatized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EldMfxda0Xyk"
      },
      "source": [
        "**Stemming**: Stemming reduces words to their root or stem form by removing suffixes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E80WOdem0bAm",
        "outputId": "204b50d1-2b3c-4c0f-ee5b-21080f8febf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i am run in the park and play with my friend .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "stemmed_text = ' '.join(stemmed_tokens)\n",
        "print(stemmed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1v4k-9_0vwb"
      },
      "source": [
        "**Part of Speech Tagging**: Part-of-speech tagging (POS tagging) is the process of assigning a part-of-speech label (such as noun, verb, adjective, etc.) to each word in a text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5kGt1-X0zX5",
        "outputId": "e28fff8e-3a41-448d-ecb1-1c3a9cd18c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('running', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('park', 'NN'), ('and', 'CC'), ('playing', 'NN'), ('with', 'IN'), ('my', 'PRP$'), ('friends', 'NNS'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTNgR1OB1SiJ"
      },
      "source": [
        "## **TextBlob**\n",
        "\n",
        "**Sentiment Analysis**: Sentiment analysis, also known as opinion mining, is a natural language processing (NLP) technique used to determine the emotional tone or sentiment expressed in a piece of text, such as a sentence, paragraph, or document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxGRtdE_1nMN",
        "outputId": "4f6e8449-0731-4857-85bb-1dff87956759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in ./venv/lib/python3.11/site-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in ./venv/lib/python3.11/site-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in ./venv/lib/python3.11/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.11/site-packages (from nltk>=3.1->textblob) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.17.1\n",
            "[nltk_data] Downloading package brown to /home/jasleen/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /home/jasleen/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/jasleen/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/jasleen/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /home/jasleen/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /home/jasleen/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uvgkfteM6d_b"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sORQuu18tU1"
      },
      "source": [
        "**Polarity Score**: The polarity score measures the sentiment of the text as positive, negative, or neutral. It is represented as a numerical value ranging from -1 (very negative) to 1 (very positive), with 0 indicating a neutral sentiment.\n",
        "\n",
        "**Subjectivity Score**: The subjectivity score measures the subjectivity or objectivity of the text. It is represented as a numerical value ranging from 0 to 1, where 0 is highly objective (factual) and 1 is highly subjective (opinionated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XQZKJ1U1uoA",
        "outputId": "68a959a5-354c-4830-b3c7-c6006fd7d15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenization:\n",
            "['TextBlob', 'is', 'a', 'simple', 'and', 'easy-to-use', 'library', 'for', 'processing', 'textual', 'data']\n",
            "\n",
            "Part-of-speech tagging:\n",
            "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('and', 'CC'), ('easy-to-use', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('processing', 'VBG'), ('textual', 'JJ'), ('data', 'NNS')]\n",
            "\n",
            "Sentiment Analysis:\n",
            "Polarity: 0.0\n",
            "Subjectivity: 0.35714285714285715\n"
          ]
        }
      ],
      "source": [
        "text = \"TextBlob is a simple and easy-to-use library for processing textual data.\"\n",
        "\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(\"\\nTokenization:\")\n",
        "print(blob.words)\n",
        "\n",
        "print(\"\\nPart-of-speech tagging:\")\n",
        "print(blob.tags)\n",
        "\n",
        "print(\"\\nSentiment Analysis:\")\n",
        "print(f\"Polarity: {blob.sentiment.polarity}\")\n",
        "print(f\"Subjectivity: {blob.sentiment.subjectivity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /home/jasleen/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVbQ82O56TSN"
      },
      "source": [
        "**Machine Translation**: Textblob uses Google Translator's API to provide a very simple interface for translating text.\n",
        "\n",
        "The error message you are getting is because the TextBlob.translate() method is deprecated. As you mentioned in your prompt, the TextBlob library has deprecated the translate() and detect_language() methods since version 0.16.0. You can use the official Google Translate API instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 400: Bad Request",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/jasleen/Desktop/Thapar/Conversational-Systems/NLP.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jasleen/Desktop/Thapar/Conversational-Systems/NLP.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextblob\u001b[39;00m \u001b[39mimport\u001b[39;00m TextBlob\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jasleen/Desktop/Thapar/Conversational-Systems/NLP.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m blob \u001b[39m=\u001b[39m TextBlob(\u001b[39m\"\u001b[39m\u001b[39mComment vas-tu?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jasleen/Desktop/Thapar/Conversational-Systems/NLP.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(blob\u001b[39m.\u001b[39;49mdetect_language())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jasleen/Desktop/Thapar/Conversational-Systems/NLP.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(blob\u001b[39m.\u001b[39mtranslate(to\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mes\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jasleen/Desktop/Thapar/Conversational-Systems/NLP.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(blob\u001b[39m.\u001b[39mtranslate(to\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m))\n",
            "File \u001b[0;32m~/Desktop/Thapar/Conversational-Systems/venv/lib/python3.11/site-packages/textblob/blob.py:597\u001b[0m, in \u001b[0;36mBaseBlob.detect_language\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Detect the blob's language using the Google Translate API.\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \n\u001b[1;32m    574\u001b[0m \u001b[39mRequires an internet connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[39m:rtype: str\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    592\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    593\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mTextBlob.detext_translate is deprecated and will be removed in a future release. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    594\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mUse the official Google Translate API instead.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    595\u001b[0m     \u001b[39mDeprecationWarning\u001b[39;00m\n\u001b[1;32m    596\u001b[0m )\n\u001b[0;32m--> 597\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtranslator\u001b[39m.\u001b[39;49mdetect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw)\n",
            "File \u001b[0;32m~/Desktop/Thapar/Conversational-Systems/venv/lib/python3.11/site-packages/textblob/translate.py:76\u001b[0m, in \u001b[0;36mTranslator.detect\u001b[0;34m(self, source, host, type_)\u001b[0m\n\u001b[1;32m     70\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mq\u001b[39m\u001b[39m\"\u001b[39m: source}\n\u001b[1;32m     71\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{url}\u001b[39;00m\u001b[39m&sl=auto&tk=\u001b[39m\u001b[39m{tk}\u001b[39;00m\u001b[39m&client=\u001b[39m\u001b[39m{client}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     72\u001b[0m     url\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl,\n\u001b[1;32m     73\u001b[0m     tk\u001b[39m=\u001b[39m_calculate_tk(source),\n\u001b[1;32m     74\u001b[0m     client\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mte\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m )\n\u001b[0;32m---> 76\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(url, host\u001b[39m=\u001b[39;49mhost, type_\u001b[39m=\u001b[39;49mtype_, data\u001b[39m=\u001b[39;49mdata)\n\u001b[1;32m     77\u001b[0m result, language \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response)\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m language\n",
            "File \u001b[0;32m~/Desktop/Thapar/Conversational-Systems/venv/lib/python3.11/site-packages/textblob/translate.py:96\u001b[0m, in \u001b[0;36mTranslator._request\u001b[0;34m(self, url, host, type_, data)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m host \u001b[39mor\u001b[39;00m type_:\n\u001b[1;32m     95\u001b[0m     req\u001b[39m.\u001b[39mset_proxy(host\u001b[39m=\u001b[39mhost, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mtype_)\n\u001b[0;32m---> 96\u001b[0m resp \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39;49murlopen(req)\n\u001b[1;32m     97\u001b[0m content \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mread()\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m content\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
            "File \u001b[0;32m/usr/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mfor\u001b[39;00m processor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response\u001b[39m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[1;32m    527\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/usr/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[39m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[1;32m    635\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[1;32m    637\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/usr/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
            "File \u001b[0;32m/usr/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/usr/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        " \n",
        "blob = TextBlob(\"Comment vas-tu?\")\n",
        " \n",
        "print(blob.detect_language())\n",
        " \n",
        "print(blob.translate(to='es'))\n",
        "print(blob.translate(to='en'))\n",
        "print(blob.translate(to='zh'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Valence Aware Dictionary and sEntiment Reasoner (VADER)** is a recently developed lexicon-based sentiment analysis tool whose accuracy is shown to be much greater than the existing lexicon-based\n",
        "sentiment analyzers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'neg': 0.556, 'neu': 0.444, 'pos': 0.0, 'compound': -0.3612}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "analyser.polarity_scores(\"This life sucks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Web Scraping Libraries and Methodology**: Web scraping is a technique for extracting information from websites. It involves automating the web browser to navigate a web page, extract data from it, and save it in a structured format. Web scraping is a useful skill for data scientists, researchers, and data enthusiasts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "titles = []\n",
        "prices = []\n",
        "ratings = []\n",
        "\n",
        "url = \"https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops\"\n",
        "\n",
        "request = requests.get(url)\n",
        "# print(request.text)\n",
        "soup = BeautifulSoup(request.text, \"html.parser\")\n",
        "\n",
        "for prod in soup.find_all(\"div\", {'class': 'col-sm-4 col-lg-4 col-md-4'}):\n",
        "    for pr in prod.find_all(\"div\", {'class': 'caption'}):\n",
        "        for p in pr.find_all('h4', {'class': 'pull-right price'}):\n",
        "            prices.append(p.text)\n",
        "        for t in pr.find_all('a', {'title'}):\n",
        "            titles.append(t.get('title'))\n",
        "    for rt in prod.find_all('div', {'class':'ratings'}):\n",
        "        ratings.append(len(rt.find_all('span', {'class':'glycophicon glycophicon-star'})))\n",
        "\n",
        "# print(prices, ratings, titles)\n",
        "prod_df = pd.DataFrame(zip(titles, prices, ratings), columns=['Titles','Prices','Ratings'])\n",
        "prod_df.to_csv(\"ecommerce.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPAW3DawcP+cho0tFoVcQyQ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
