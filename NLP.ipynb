{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbw2eAIVGjVbsmwioAcrff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jasleen8801/Conversational-Systems/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLTK**\n",
        "The Natural Language Toolkit (NLTK) is a powerful Python library for working with human language data, particularly for natural language processing (NLP) and text analysis tasks."
      ],
      "metadata": {
        "id": "PyPKEXm83CIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClrB7_UexJgT",
        "outputId": "720fcd10-379b-4794-f8c3-569eebe2ed83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK Corpora**: NLTK Corpora refers to a collection of text datasets and resources that the Natural Language Toolkit (NLTK) library provides.\n",
        "\n",
        "**Text Processing**: Text Processing involves converting textual data into mathematical objects, typically vectors or matrices, that can be analyzed, manipulated, and used for various machine learning and statistical techniques."
      ],
      "metadata": {
        "id": "pZ5MWhk_ya-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjsgGxi9ze00",
        "outputId": "f50c270f-32ee-4178-9fd0-91fe4fbd9b81"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am running in the park and playing with my friends.\""
      ],
      "metadata": {
        "id": "7LMqa6O83mjB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**: Tokenization is a fundamental text processing task in natural language processing (NLP). It involves breaking a text into individual units, typically words or tokens."
      ],
      "metadata": {
        "id": "ZygyOIzM3d2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhOxN2gmxorA",
        "outputId": "2fb12ee0-6903-4994-93f4-a1c85554f968"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'running', 'in', 'the', 'park', 'and', 'playing', 'with', 'my', 'friends', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop words removal**-  Stop words are words that are considered to be of little value in text analysis because they are very common and do not carry much information about the content of the text. Examples of stop words in English include \"the,\" \"and,\" \"in,\" \"is,\" and \"it.\" Removing stop words can help reduce noise in your text data and improve the efficiency and accuracy of various NLP tasks."
      ],
      "metadata": {
        "id": "qIYS4Ek7zJc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "\n",
        "# print(tokens)\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "filtered_text = ' '.join(filtered_tokens)\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIkxHhLFzN5i",
        "outputId": "7faf4840-2a3b-4bc2-ee9c-e26c215f27e0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'themselves', 'because', 'are', 'whom', \"haven't\", 'can', 'just', 'by', 'from', 'herself', 'has', 'with', 'above', \"it's\", 'been', 'all', \"hasn't\", 'further', 'about', \"couldn't\", 'ma', 'wasn', \"mightn't\", 'mustn', 'at', 'most', 'hasn', 'll', \"wasn't\", 'were', 'weren', 'theirs', 'only', 'her', 'was', 't', 'him', 'through', 're', 'between', 'there', \"doesn't\", \"wouldn't\", 'my', \"you'll\", 'why', 'me', 'them', \"that'll\", 'too', 'himself', 'isn', 'as', 'again', 'no', 'is', 'off', \"weren't\", 'who', 'but', 'don', 'you', \"hadn't\", 'on', 'very', 'have', 'those', \"should've\", 'this', 'y', \"isn't\", 'they', 'an', 'not', 'same', 'didn', 'needn', 'until', 'against', 'yours', 'will', \"you're\", 'then', 'did', 'i', 'for', 'she', \"shan't\", 'during', 'more', \"don't\", 'and', 'where', \"you've\", 'below', 'wouldn', 'both', 'or', 'than', 'up', 'these', 'does', 'once', 'he', \"you'd\", 'mightn', \"aren't\", 'yourselves', 'be', 'ourselves', 'our', 'am', 'shan', 'before', 'which', 'doing', 'won', 'that', 'such', 'the', 'nor', 'out', 'o', 'their', 'having', 'here', 'other', 'now', 'few', 'm', 'any', 'own', 'his', 'some', \"needn't\", \"won't\", 'do', 'after', 'doesn', \"shouldn't\", 'each', 'to', 'aren', 'down', 'had', 'if', 'into', \"she's\", 'ours', 'couldn', 'while', 'a', 'itself', 've', 'd', \"didn't\", 'haven', 'so', 's', \"mustn't\", 'myself', 'your', 'how', 'when', 'under', 'should', 'of', 'we', 'hers', 'it', 'what', 'in', 'over', 'yourself', 'hadn', 'shouldn', 'its', 'being', 'ain'}\n",
            "running park playing friends .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**: Lemmatization reduces words to their base or root form, which can be helpful in various natural language processing tasks."
      ],
      "metadata": {
        "id": "qmtRwKGVz73O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uny7JJKVzhRW",
        "outputId": "ff34f027-5bb4-44f3-fa6e-5950120be5fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am running in the park and playing with my friend .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**: Stemming reduces words to their root or stem form by removing suffixes."
      ],
      "metadata": {
        "id": "EldMfxda0Xyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "stemmed_text = ' '.join(stemmed_tokens)\n",
        "print(stemmed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E80WOdem0bAm",
        "outputId": "204b50d1-2b3c-4c0f-ee5b-21080f8febf7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am run in the park and play with my friend .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of Speech Tagging**: Part-of-speech tagging (POS tagging) is the process of assigning a part-of-speech label (such as noun, verb, adjective, etc.) to each word in a text.\n"
      ],
      "metadata": {
        "id": "s1v4k-9_0vwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5kGt1-X0zX5",
        "outputId": "e28fff8e-3a41-448d-ecb1-1c3a9cd18c47"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('running', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('park', 'NN'), ('and', 'CC'), ('playing', 'NN'), ('with', 'IN'), ('my', 'PRP$'), ('friends', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TextBlob**\n",
        "\n",
        "**Sentiment Analysis**: Sentiment analysis, also known as opinion mining, is a natural language processing (NLP) technique used to determine the emotional tone or sentiment expressed in a piece of text, such as a sentence, paragraph, or document.\n",
        "\n",
        "**Polarity Score**: The polarity score measures the sentiment of the text as positive, negative, or neutral. It is represented as a numerical value ranging from -1 (very negative) to 1 (very positive), with 0 indicating a neutral sentiment.\n",
        "\n",
        "**Subjectivity Score**: The subjectivity score measures the subjectivity or objectivity of the text. It is represented as a numerical value ranging from 0 to 1, where 0 is highly objective (factual) and 1 is highly subjective (opinionated)."
      ],
      "metadata": {
        "id": "OTNgR1OB1SiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxGRtdE_1nMN",
        "outputId": "4f6e8449-0731-4857-85bb-1dff87956759"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"TextBlob is a simple and easy-to-use library for processing textual data.\"\n",
        "\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(\"\\nTokenization:\")\n",
        "print(blob.words)\n",
        "\n",
        "print(\"\\nPart-of-speech tagging:\")\n",
        "print(blob.tags)\n",
        "\n",
        "print(\"\\nSentiment Analysis:\")\n",
        "print(f\"Polarity: {blob.sentiment.polarity}\")\n",
        "print(f\"Subjectivity: {blob.sentiment.subjectivity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XQZKJ1U1uoA",
        "outputId": "4ae393ab-e26e-4df3-f657-1e06642e2727"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenization:\n",
            "['TextBlob', 'is', 'a', 'simple', 'and', 'easy-to-use', 'library', 'for', 'processing', 'textual', 'data']\n",
            "\n",
            "Part-of-speech tagging:\n",
            "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('and', 'CC'), ('easy-to-use', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('processing', 'VBG'), ('textual', 'JJ'), ('data', 'NNS')]\n",
            "\n",
            "Sentiment Analysis:\n",
            "Polarity: 0.0\n",
            "Subjectivity: 0.35714285714285715\n"
          ]
        }
      ]
    }
  ]
}