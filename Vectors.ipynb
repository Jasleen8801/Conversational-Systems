{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computers': 5, 'can': 4, 'analyze': 1, 'text': 18, 'to': 20, 'find': 9, 'sentiment': 17, 'but': 3, 'they': 19, 'don': 8, 'really': 16, 'understand': 21, 'meaning': 13, 'do': 7, 'it': 10, 'using': 22, 'vectors': 23, 'and': 2, 'matrices': 12, 'process': 15, 'massive': 11, 'amounts': 0, 'of': 14, 'data': 6}\n",
      "[[0 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1]\n",
      " [1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X = (\n",
    "    \"Computers can analyze text to find sentiment, but they don't really understand meaning. \",\n",
    "    \"They do it using vectors and matrices\",\n",
    "    \"Computers can process massive amounts of text data\",\n",
    ")\n",
    "vectorizer = CountVectorizer()\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(X_vec.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jasleen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jasleen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'day', 'every', 'evolving', 'field', 'language', 'making', 'natural', 'processing', 'reading', 'understand']\n",
      "[[0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 2. 1. 1. 1. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# BoW matrix from scratch using NLTK\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    texts,\n",
    "    keep_list=[],\n",
    "    stemming=False,\n",
    "    stem_type=None,\n",
    "    lemmatization=False,\n",
    "    remove_stopwords=False,\n",
    "):\n",
    "    if stemming:\n",
    "        if stem_type == \"Porter\":\n",
    "            stemmer = PorterStemmer()\n",
    "        elif stem_type == \"Snowball\":\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid stemmer type\")\n",
    "    else:\n",
    "        stemmer = None\n",
    "\n",
    "    if lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    else:\n",
    "        lemmatizer = None\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        for word in keep_list:\n",
    "            stop_words.discard(word)\n",
    "\n",
    "    preprocessed_text = []\n",
    "\n",
    "    for text in texts:\n",
    "        words = re.findall(r\"\\w+\", text.lower())\n",
    "        if stemming and stem_type:\n",
    "            words = [stemmer.stem(word) for word in words]\n",
    "        elif lemmatization:\n",
    "            words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "        if remove_stopwords:\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        preprocessed_text.append(\" \".join(words))\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "common_dot_words = [\"U.S.\", \"U.N.\", \"E.U.\", \"U.K.\"]\n",
    "\n",
    "sentences = [\n",
    "    \"We are reading about Natural Language Processing Here\",\n",
    "    \"Natural Language Processing making computers understand language\",\n",
    "    \"The field of Natural Language Processing is evolving every day\",\n",
    "]\n",
    "\n",
    "sentence_list = list(sentences)\n",
    "\n",
    "preprocessed_corpus = preprocess(\n",
    "    sentence_list,\n",
    "    keep_list=common_dot_words,\n",
    "    stemming=False,\n",
    "    stem_type=None,\n",
    "    lemmatization=True,\n",
    "    remove_stopwords=True,\n",
    ")\n",
    "# print(preprocessed_corpus)\n",
    "\n",
    "set_of_words = set()\n",
    "for sentence in preprocessed_corpus:\n",
    "    for word in sentence.split():\n",
    "        set_of_words.add(word)\n",
    "vocab = list(set_of_words)\n",
    "vocab.sort()\n",
    "print(vocab)\n",
    "\n",
    "position = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))\n",
    "for i, sentence in enumerate(preprocessed_corpus):\n",
    "    for word in sentence.split():\n",
    "        bow_matrix[i][position[word]] += 1\n",
    "print(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reading': 9, 'natural': 7, 'language': 5, 'processing': 8, 'making': 6, 'computer': 0, 'understand': 10, 'field': 4, 'evolving': 3, 'every': 2, 'day': 1}\n",
      "[[0 0 0 0 0 1 0 1 1 1 0]\n",
      " [1 0 0 0 0 2 1 1 1 0 1]\n",
      " [0 1 1 1 1 1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Using CountVectorizer for BoW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reading': 27, 'natural': 19, 'language': 12, 'processing': 22, 'reading natural': 28, 'natural language': 20, 'language processing': 13, 'reading natural language': 29, 'natural language processing': 21, 'making': 16, 'computer': 0, 'understand': 30, 'processing making': 25, 'making computer': 17, 'computer understand': 1, 'understand language': 31, 'language processing making': 15, 'processing making computer': 26, 'making computer understand': 18, 'computer understand language': 2, 'field': 9, 'evolving': 6, 'every': 4, 'day': 3, 'field natural': 10, 'processing evolving': 23, 'evolving every': 7, 'every day': 5, 'field natural language': 11, 'language processing evolving': 14, 'processing evolving every': 24, 'evolving every day': 8}\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 2 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1]\n",
      " [0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Using CountVectorizer for BoW with n-grams\n",
    "\n",
    "vectorizer_ngram_range = CountVectorizer(analyzer=\"word\", ngram_range=(1, 3))\n",
    "bow_matrix_ngram = vectorizer_ngram_range.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_ngram_range.vocabulary_)\n",
    "print(bow_matrix_ngram.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 2, 'language': 0, 'processing': 5, 'natural language': 3, 'language processing': 1, 'natural language processing': 4}\n",
      "[[1 1 1 1 1 1]\n",
      " [2 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Using CountVectorizer for BoW with max_features\n",
    "\n",
    "vectorizer_max_features = CountVectorizer(\n",
    "    analyzer=\"word\", ngram_range=(1, 3), max_features=6\n",
    ")\n",
    "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_max_features.vocabulary_)\n",
    "print(bow_matrix_max_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reading': 9, 'natural': 7, 'language': 5, 'processing': 8, 'making': 6, 'computer': 0, 'understand': 10, 'field': 4, 'evolving': 3, 'every': 2, 'day': 1}\n",
      "[[0.         0.         0.         0.         0.         0.41285857\n",
      "  0.         0.41285857 0.41285857 0.69903033 0.        ]\n",
      " [0.4431132  0.         0.         0.         0.         0.52341958\n",
      "  0.4431132  0.26170979 0.26170979 0.         0.4431132 ]\n",
      " [0.         0.44514923 0.44514923 0.44514923 0.44514923 0.26291231\n",
      "  0.         0.26291231 0.26291231 0.         0.        ]]\n",
      "\n",
      " The shape of TF-IDF matrix is:  (3, 11)\n"
     ]
    }
   ],
   "source": [
    "# Using TF-IDF for BoW\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(tfidf_matrix.toarray())\n",
    "print(\"\\n The shape of TF-IDF matrix is: \", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reading': 9, 'natural': 7, 'language': 5, 'processing': 8, 'making': 6, 'computer': 0, 'understand': 10, 'field': 4, 'evolving': 3, 'every': 2, 'day': 1}\n",
      "[[0.         0.         0.         0.         0.         0.21307663\n",
      "  0.         0.21307663 0.21307663 0.3607701  0.        ]\n",
      " [0.18648142 0.         0.         0.         0.         0.22027787\n",
      "  0.18648142 0.11013893 0.11013893 0.         0.18648142]\n",
      " [0.         0.17325473 0.17325473 0.17325473 0.17325473 0.10232703\n",
      "  0.         0.10232703 0.10232703 0.         0.        ]]\n",
      "\n",
      " The shape of TF-IDF matrix is:  (3, 11)\n"
     ]
    }
   ],
   "source": [
    "# Using TF-IDF for BoW with l1 norm\n",
    "\n",
    "vectorizer_l1_norm = TfidfVectorizer(norm=\"l1\")\n",
    "tfidf_matrix_l1_norm = vectorizer_l1_norm.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_l1_norm.vocabulary_)\n",
    "print(tfidf_matrix_l1_norm.toarray())\n",
    "print(\"\\n The shape of TF-IDF matrix is: \", tfidf_matrix_l1_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 2, 'language': 0, 'processing': 5, 'natural language': 3, 'language processing': 1, 'natural language processing': 4}\n",
      "[[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n",
      " [0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
      " [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]\n",
      "\n",
      " The shape of TF-IDF matrix is:  (3, 6)\n"
     ]
    }
   ],
   "source": [
    "# Using TF-IDF for BoW with n-grams, l2 norm and max_features\n",
    "\n",
    "vectorizer_ngram_max_features = TfidfVectorizer(\n",
    "    norm=\"l2\", analyzer=\"word\", ngram_range=(1, 3), max_features=6\n",
    ")\n",
    "tfidf_matrix_ngram_max_features = vectorizer_ngram_max_features.fit_transform(\n",
    "    preprocessed_corpus\n",
    ")\n",
    "print(vectorizer_ngram_max_features.vocabulary_)\n",
    "print(tfidf_matrix_ngram_max_features.toarray())\n",
    "print(\"\\n The shape of TF-IDF matrix is: \", tfidf_matrix_ngram_max_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the first document and the 1th document is 1.0\n",
      "The cosine similarity between the first document and the 2th document is 0.6666666666666666\n",
      "The cosine similarity between the first document and the 3th document is 0.5669467095138409\n"
     ]
    }
   ],
   "source": [
    "# Implementing cosine similarity\n",
    "\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "    return np.dot(vector1, vector2) / (\n",
    "        np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2))\n",
    "    )\n",
    "\n",
    "\n",
    "for i in range(bow_matrix.shape[0]):\n",
    "    print(\n",
    "        f\"The cosine similarity between the first document and the {i+1}th document is {cosine_similarity(bow_matrix[0], bow_matrix[i])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from seaborn import load_dataset\n",
    "\n",
    "df = load_dataset('penguins')\n",
    "ohe = OneHotEncoder()\n",
    "transformed = ohe.fit_transform(df[['island']])\n",
    "print(transformed.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jasleen, I am a chatbot. You can ask me questions about electronics.\n",
      "so far after i charge the battery it will last about 90 minutes. i have not had any issues with the battery.\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# Basic Chatbot\n",
    "\n",
    "import ast\n",
    "import gzip\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "with gzip.open('qa_Electronics.json.gz', 'rt', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = ast.literal_eval(line)\n",
    "        questions.append(data['question'].lower())\n",
    "        answers.append(data['answer'].lower())\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(questions)\n",
    "tfidf = TfidfTransformer(norm='l2')\n",
    "X_tfidf = tfidf.fit_transform(X_vec)\n",
    "\n",
    "def conversation(im):\n",
    "    global tfidf, answers, X_tfidf\n",
    "    Y_vec = vectorizer.transform([im])\n",
    "    Y_tfidf = tfidf.transform(Y_vec)\n",
    "    angle = np.rad2deg(np.arccos(max(cosine_similarity(Y_tfidf, X_tfidf)[0])))\n",
    "    if angle > 60:\n",
    "        return \"Sorry, I don't understand.\"\n",
    "    else:\n",
    "        return answers[np.argmax(cosine_similarity(Y_tfidf, X_tfidf)[0])]\n",
    "\n",
    "def main():\n",
    "    usr = input(\"Enter your username: \")\n",
    "    print(f\"Hi {usr}, I am a chatbot. You can ask me questions about electronics.\")\n",
    "    while True:\n",
    "        im = input(\"Enter your question: \")\n",
    "        if im == 'exit':\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        print(conversation(im))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
